\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}

\begin{document}

\title{Concurrent Programming Languages}

\author{
Alexis Beingessner
\and
Troy Hildebrandt
}

\maketitle

\section{Abstract}

Developing applications for distributed systems provides unique challenges that
many modern programming languages are not inherently equipped for. These
challenges are often ones that arise with parallel and concurrent programming,
with additional concerns such as node and network failures, software
survivability, and network latency issues, among others.

Certain languages attempt to tackle some of the major problems associated with
developing distributed applications. These languages provide built-in facilities
for creating nodes that communicate over a network, error handling, message
passing, and mechanisms for more transparently simulating shared memory. This
simplifies fault-tolerance for synchronous access to data across multiple
machines.

We look at several languages and their respective extensions that enable
distributed application development, including Erlang, Concurrent/Cloud Haskell,
and Ada. For each, we examine their strengths and innovations, and assess their
impact on distributed computing. The languages explored all provide unique
solutions to some of distributed programming's toughest problems, in some cases
using completely different programming paradigms.

\section{Introduction}

A system can generally be implemented to run in four basic ways:
sequentially, concurrently, in parallel, or distributed.

In a sequential system steps logically execute in a fixed order. Sequential
systems have long dominated the programming landscape as easy to implement and
reason about.  that dominate the programming landscape. However for many
problems a sequential implementation has undesirable performance or design
implications. If multiple tasks need to be performed, steps require particular
real-world resources, or must grapple with the non-sequential nature of the real
world, then sequential operation can be far from optimal.

Concurrent programming involves allowing some operations to logically occur in
arbitrary, potentially overlapping, order. For instance, it is generally
desirable to be able to handle multiple TCP connections, even if one does not
have the resources to actively perform work for them all. Even if only one
connection is actually being worked on at a time, allowing each connection to be
handled concurrently allows us to perform productive work \emph{more often} than
sequential connection handling: if one connection is blocked on waiting for a
response or system resource, another connection can be worked on. Concurrency
can be implemented on top of a logically sequential system using time slicing,
though this limits the performance that can be realized. CPU-bound applications
will see little to no benefit from such a system.

Parallel programming involves actually performing operations at the same time.
Parallel programming is most easily enabled by concurrent programming. If a
program is already concurrent, then concurrent tasks can simply be driven
forward at once. A sequential system can be built on top of a parallel system
through synchronization mechanisms (simplifying reasoning while reducing the
value of the parallel system), but a parallel system cannot be built on top of a
sequential one.

Distributed programming involves actually performing operations across several
otherwise independent machines. This enables computation to be done at a greater
scale than a single machine could ever hope to do with current technology. It
also allows applications to be made robust against hardware failures, while
ironically increasing the probability of such a failure. Single-machine
applications can often ignore the consequences of hardware failures as unlikely
or at worst properly handled by the underlying operating system. However the
ultimate consequence of no longer running can itself be an unacceptable.

Distributed computing can itself be decomposed into two sub-domains: cluster
computing and global computing. The distinction largely being exactly how far
apart the systems are. Clusters are co-located, enabling faster communication
and tighter control over the system. Global systems are distributed across the
globe, increasing the cost of intra-communication while making the system more
available to third parties and more robust to local disasters. For the purposes
of this work, these distinctions will not be particularly important.

Most hardware, operating systems, and programming languages readily permit an
obvious sequential implementation. In particular C was developed in a world of
single-process, single-user, CPU-bound systems, and its highly sequential and
computation-oriented nature reflects this. However modern hardware
and operating systems are highly parallel and significantly more powerful.
Applications are expected to serve millions of logically concurrent and IO-bound
requests on datasets orders of magnitude larger than a single system could
handle. These problems \emph{necessitate} a distributed system. While it's not
\emph{impossible} to get C to efficiently handle these problems, it's certainly
error-prone and difficult. For this reason we argue that C is simply
\emph{inappropriate} for distributed systems.

But C is \emph{the} systems programming language. Its only real competition is
an increasingly complex pseudo-superset of itself (C++). Therefore we are left
asking what programming languages \emph{are} appropriate for distributed systems
to be implemented in.

This paper surveys the programming language literature for how different
languages enable efficient and correct distributed systems to be more easily
developed. In particular we observe that functional languages with support for
\emph{concurrent} programming allow systems to be more trivially parallelized.
When combined with an emphasis on message passing over shared state, tasks can
then be made distributed without significant semantic changes over a single
system deployment. In fact, one can deploy and debug a distributed system on a
single machine accurately. However message passing alone is insufficient.
Communication and task handling must be done in a fault-tolerant manner. Tasks
cannot be assumed to receive a message or even consistently \emph{exist}.

\section{Parallel, Concurrent, Distributed}
The differences between parallel, concurrent, and distributed programming are
subtle at times, and they all provide many of the same challenges. Parallel and
concurrent programming are more similar in that they provide the ability to run
multiple processes in unison on a single machine, and many current programming
languages provide the ability to program both parallel and concurrent
applications. Similar issues arise regardless of whether parallelism,
concurrency, or a distributed system are desired. Developing software that
breaks functionality into independently executable threads or tasks is often not
trivial, and for any of the above programming models this is an absolute
necessity.

However, distributed programming brings with it several new difficulties that
the single system programming models don't face. In concurrent programming for
instance, the cost of information transfer between processes is minimal since
the transfer is done locally, whereas in distributed programming this is much
more expensive, and even potentially impossible due to network conditions.
Distributed programming also has to deal with graceful failures as nodes go down
in potentially distant locations, whereas there is no such thing as a partial
failure in a concurrent programming environment. With concurrent programming, ..
% MORE.

\section{Erlang}

Erlang is a primarily functional language that was designed to solve the
problems faced by the telecom industry. Telecom systems are expected to meet the
following requirements \cite{dacker2000concurrent}:

\begin{itemize}
    \item The system must be able to handle very large numbers of concurrent activities.
    \item Actions must be performed at a certain point in time or within a certain time.
    \item Systems may be distributed over several computers.
    \item The system is used to control hardware.
    \item The software systems are very large.
    \item The systems should be in continuous operation for many years.
    \item Software maintenance (reconfiguration, etc) should be performed without stopping the system.
    \item There are stringent quality, and reliability requirements.
    \item Fault tolerance both to hardware failures, and software errors, must be provided.
\end{itemize}

Erlang's primary tool for handling these problems is its \emph{process} system.
\cite{erlangthesis} An Erlang process is a lightweight task that is logically
isolated from all other processes. They run concurrently, and can only
communicate by passing messages to processes whose secret ids they know.

An Erlang program is intended to be decomposed into many small processes. Further,
processes are often broken up into a hierarchy of service quality to be attempted in a
best-effort manner. That is, the highest quality service is always preferred, but if
that fails an easier but less-good process is tried instead. This helps ensure
that even in face of hardware and software errors, the system is able to
maintain some basic level of operation.

The model of computation for \emph{individual} processes is that they're
expected to try to do the right thing, but are considered unreliable. One should
be prepared for tasks to experience an error or disappear completely at any
time. Messages sent to a task cannot be assumed to be received; a response must
be explicitly sent back to confirm that a message was sent and handled
successfully.

Erlang's processes system does its best to ensure that a flawed process cannot
harm the execution of a correct process unless the correct one specifically opts
into such a dependency. A process is expected to fail as soon as a problem is
identified, rather than try to patch over the issue. Supervisor processes handle
the administrative problem of how to handle problem. In this way developers are
able to separate the business logic and the administrative logic.
\cite{erlangADX}

Processes can also have their code updated at runtime. This can be done fairly
cleanly due to the functional nature of Erlang: loops are generally encoded as
recursion, and so any code that is run is likely to be re-entered by a function
call which can be redirected to the new version without halting execution. It is
otherwise the programmer's responsibility to ensure that new code is compatible
with the old running code. \cite{erlangthesis}

Erlang's processes enable it to reasonably meet all the requirements for a
telecom system. As such it saw wide deployment at Ericsson and Nortel. Notably
Erlang provided the backbone of the Ericsson AXD301 switching system, whose
developers reported a substantial improvement in productivity and reduction in
bugs. \cite{erlangADX} One deployment of this system reported 9 9's
(99.9999999\%) of reliability on an 11 node deployment. \cite{erlangthesis}

In the same vein as Java and unlike C, Erlang is built to abstract away many of
the implementation details of hardware. This enables Erlang applications and
libraries to be ported and distributed to heterogeneous hardware with little
fuss or muss. \cite{erlangthesis}

\section{Cloud Haskell}

From Microsoft Research comes Cloud Haskell, a set of libaries that ease the
creation of distributed applications wtih the Haskell programming language in
an attempt to solve many of the problems with creating distributed applications.
Although Haskell provides the necessary tools to develop parallel and concurrent
applications natively, distributed programming has its own set of requirements
and challenges that it isn't equipped to handle. Cloud Haskell provides a
collection of facilities to deal with the issues surrounding the interprocess
communication difficulties of a multi-node system communicating over a
network, and graceful failure resolution behaviour for partial faillures due
to node outages that don't occur in single system environments.

In any distributed system, there must be some level of communication between
processes running on many different machines, and one of the most costly
operations in such a system is the transfer of data between nodes when
interprocess communication is necessary. Cloud Haskell employs a message passing
system inspired by Erlang wherein no processes have access to each others' data,
and this information must be explicitly sent between processes if they are to
share it.

Use of the Erlang message passing method here is beneficial for a variety of
reasons; if messages must be explicitly passed between processes, there is an
explicit cost to the communication which may drive developers to rely less on
interprocess communication and look to structuring their computation differently
to minimize costly message passing. When processes have to send messages before
data is shared, it is also possible to avoid the corruption or contamination of
data in one process by another, ensuring that failure in one process does not
result in the failure of another process.

Although Cloud Haskell provides message passing like that of Erlang, which in
this case are referred to as \emph{untyped messages}, it also provides something
unique known as \emph{typed channels}. Unliked untyped messages, which are sent
directly using the process identifier, typed channels have a send and receive
port. The receive port in this case will only accept messages of a certain type,
which gives developers a guarantee that the function will only have to deal with
messages of a type that it expects. The benefit to this is that, using Haskell's
strong typing, one can ensure at development time that a message will be
accepted by the receiving process, something that can't be determined until
runtime using untyped messages.

One of the things that makes Cloud Haskell unique is that it's is built on
Haskell, a purely functional programming language. The purity of the functional
programming model used by Haskell ensures that the data in applications written
with it is immutable, and therefore there is no need for shared memory between
functions. Another major benefit is reduction in complexity from the removal of
the mutexes and semaphores required for thread and process data safety in
traditional systems programming languages like C.

Since Cloud Haskell is meant to be used for distributed systems, there is always
the possibility that one or many of the machines that make up the system will
fail at some point. The expectation, like with any distributed system, is that
the system will be fault-tolerant enough to continue functioning. An additional
benefit to the functional programming model and the lack of shared memory
between functions is that since functions do computation in an isolated fashion,
the only thing necessary in the event of a machine failure is to move the
computation of that function to another machine and continue. Since functional
programming languages don't rely on maintaining shared state between functions
as is prevalent in traditional procedural programming languages, there is no
need to rewind spurious changes to state made by faulty computations from a
failing machine.

Cloud Haskell draws on concepts from Erlang for much of its inspiration, and
fault-tolerance is one of them. Like Erlang, Cloud Haskell subscribes to the
idea that if something is going wrong in a process it's best to just terminate
it and start another one. Like Erlang, it provides a mechanism for processes to
monitor each other, so that in the event that a process does fail, there's
another process capable of terminating it and moving it. Like in Erlang, it is
possible to link two processes to bidirectionally monitor themselves.

%TODO where is it used?
%TODO talk about how it's still very new, unfinished.
%TODO citations.

\section{Ada}

Popular sequential languages such as C and C++ provide no built-in support for
concurrent programming. Ada is a language that is actually designed to deal with
concurrency natively,
% Literature on Ada sucks it seems like.

\section{C}

C is the dominant language in non-distributed systems programming. The Windows,
Linux, and OSX kernels are all written in C, and this is unlikely to ever
change. As such, it seems natural to consider C (and close derivatives like C++)
for the distributed systems. Part of the value of C is the massive preexisting
code- and knowledge-base surrounding it. If getting C to work with distributed
systems requires programming in a completely different way, then these
advantages cannot be effectively leveraged. Especially if it means that legacy C
code does not inter-operate with distributed C code.

Unfortunately this seems to be the case. Idiomatic C programs openly make use of
shared memory and aliased mutation through pointers. Indeed, pointers are often
the *cheapest* way to give a value to someone else. This poses a problem because
pointers are of course completely nonsensical values between two independent
machines.

Concert/C \cite{auerbach1994concert} is a superset of C that tries to handle
this problem. When a value that contains a pointer is passed to another system,
it performs a *deep* copy of the value through all of the pointers. This proves
problematic for two reasons. First, this forms a mismatch between traditional C
programming and distributed C programming: passing and composing pointers is
normally super cheap, but in Concert/C is potentially incredibly expensive.
Second, substantial annotations are required by the programmer to identify
substantial details about the semantics of the pointers being copied. Whether
pointers are nullable or aliasable, who owns the data, and whether it's an array
and how long it is are all details that the programmer must be able to tell
Concert/C about that otherwise wouldn't be necessary in normal C.

The functional languages we surveyed avoid the annotation problems largely due
to pervasive garbage collection and immutability. The runtime either already
knows the answer to these questions or renders the answer irrelevant (ownership
and aliasing is irrelevant for immutable garbage-collected data). Making
transfers clear through explicit message passing also means that the functional
languages better communicate high cost operations. In Concert/C a normal-looking
function call may be executed on a remote system transparently.

C has a generally *trusted* model for computation. Hardware is assumed to be
reliable and function calls are expected to correctly return a valid result.
However this is no longer the case in the distributed context. Hardware is
expected to fail and software might crash, but there is no way for a caller of a
C function to see or handle this. Once again the explicit message-passing
boundary in the functional languages makes this clear.

The pervasive shared mutability and state may also be hidden through globals or
deeply nested pointers. Operations which seem simple and behave efficiently on a
single machine may involve lots of synchronization when distributed across
multiple machines, and this may not be obvious to the programmer. Especially
when linking against third-party code.


\section{Conclusion}





\addcontentsline{toc}{chapter}{References}
\small

\bibliographystyle{abbrv}
\bibliography{bibliography}

\end{document}
